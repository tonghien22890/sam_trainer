# Hybrid Conservative BÃ¡o SÃ¢m Model - Solution Analysis

## ğŸ¯ Tá»•ng quan Solution

**Hybrid Conservative Model** lÃ  giáº£i phÃ¡p káº¿t há»£p 2 lá»›p:
1. **Rulebase Layer**: Cháº·n bÃ i yáº¿u trÆ°á»›c khi ML xá»­ lÃ½
2. **ML Layer**: Decision Tree há»c pattern tá»« synthetic data

```mermaid
graph TD
    A[Input: sammove_sequence] --> B[Rulebase Check]
    B --> C{Is Weak Hand?}
    C -->|Yes| D[Block Declaration]
    C -->|No| E[ML Feature Extraction]
    E --> F[Decision Tree Prediction]
    F --> G[Conservative Threshold]
    G --> H[Final Decision]
```

## ğŸ›¡ï¸ Rulebase Layer - Weak Hand Detection

### Weak Hand Rules Configuration
```python
weak_hand_rules = {
    'required_total_cards': 10,      # Sequence pháº£i Ä‘á»§ 10 lÃ¡
    'max_weak_combos': 2,            # Tá»‘i Ä‘a 2 combo yáº¿u (strength < 0.5)
    'min_strong_combos': 1,          # Pháº£i cÃ³ Ã­t nháº¥t 1 combo máº¡nh (strength >= 0.7)
    'min_avg_strength': 0.6,         # Trung bÃ¬nh strength pháº£i >= 0.6
    'min_high_ranks': 1,             # Pháº£i cÃ³ Ã­t nháº¥t 1 combo rank >= 8
}
```

### Rule Validation Logic
1. **Card Count Rule**: Sequence pháº£i Ä‘á»§ 10 lÃ¡
2. **Weak Combo Limit**: Tá»‘i Ä‘a chiÌ‰ coÌ 1 combo cÃ³ strength < 0.5
3. **Strong Combo Requirement**: Ãt nháº¥t 1 combo cÃ³ strength >= 0.7
4. **Average Strength**: Trung bÃ¬nh strength >= 0.6
5. **High Rank Requirement**: Ãt nháº¥t 1 combo cÃ³ rank >= 8
6. **Single Combo Rule**: Náº¿u chá»‰ 1 combo thÃ¬ pháº£i strength >= 0.8

## ğŸ§  ML Layer - Decision Tree

### Feature Engineering (34 dimensions)
```
[0] Sequence length (1 dim)
[1-5] First combo type one-hot (5 dims: single, pair, triple, straight, quad)
[6-10] Second combo type one-hot (5 dims)
[11-15] Third combo type one-hot (5 dims)
[16-18] Rank values normalized (3 dims: first 3 combos)
[19-21] Combo strengths (3 dims: first 3 combos)
[22-27] Statistics (6 dims):
  - max_strength, min_strength, avg_strength
  - max_rank, min_rank, avg_rank
[28-33] Pattern indicators (6 dims):
  - strong_start, strong_finish
  - ascending, descending
  - strong_count, high_rank_count
```

### Decision Tree Configuration
```python
DecisionTreeClassifier(
    max_depth=12,           # Increased from 6 â†’ 12 for better learning
    min_samples_split=10,   # Decreased from 50 â†’ 10
    min_samples_leaf=5,     # Decreased from 25 â†’ 5
    criterion='entropy',     
    class_weight={0:1, 1:2}, # Balanced from 1:10 â†’ 1:2
    random_state=42
)
```

## ğŸ’ª Combo Strength Calculation

### Sam-Specific Strength Tiers

#### Singles
- **2 (rank=12)**: 1.0 (absolute strongest)
- **A (rank=11)**: 0.3 (weak in singles)
- **Others (3-K)**: 0.1 (weakest)

#### Pairs
- **2 (rank=12)**: 1.0
- **A (rank=11)**: 0.8
- **Others**: 0.2 + scaling

#### Triples
- **2 (rank=12)**: 1.0
- **A (rank=11)**: 0.9
- **Faces (J/Q/K)**: 0.8
- **â‰¥7 (rankâ‰¥4)**: 0.5
- **Others (3-6)**: 0.3 + scaling

#### Quads
- **2 (rank=12)**: 1.0 (absolute)
- **A (rank=11)**: 0.98
- **Others**: 0.95 + scaling

#### Straights (Most Complex)
- **10-card straight**: 1.0 (Sáº£nh rá»“ng)
- **Ace-touching**: 1.0 (A-high straights)
- **7+ cards**: 0.85+ (very strong)
- **6 cards**: 0.6 + with bonuses
- **5 cards**: 0.4 + with bonuses
- **3-4 cards**: 0.3 + with bonuses

## ğŸ¯ Hybrid Prediction Logic

### Step-by-Step Process

#### Step 1: Rulebase Filtering
```python
is_weak, weak_reason = self.is_weak_hand(sequence)
if is_weak:
    return {'should_declare': False, 'reason': f'weak_hand_blocked_{weak_reason}'}
```

#### Step 2: ML Prediction
```python
features = self.extract_enhanced_features(record)
prediction = self.model.predict(X)[0]
probabilities = self.model.predict_proba(X)[0]
confidence = max(probabilities)
```

#### Step 3: Dynamic Threshold
```python
has_super_strong = check_for_super_strong_combos(sequence)
threshold = 0.7 if has_super_strong else 0.8

should_declare = (prediction == 1 and 
                 confidence >= threshold and 
                 probabilities[1] > threshold)
```

#### Step 4: Optimal Sequence Generation
Náº¿u quyáº¿t Ä‘á»‹nh bÃ¡o, táº¡o optimal combo sequence:
- **Strategy**: Strong combos first, weak combos last
- **Logic**: Maximize early wins, minimize risk

## ğŸ“Š Training Data Analysis

### Dataset Characteristics
- **Total samples**: 1,500 records
- **Success rate**: 70.1% (1,052 success / 448 fail)
- **Feature dimensions**: 34
- **Data source**: Synthetic with success_probability rules

### Key Patterns from Analysis
```
Success vs Fail Feature Differences:
- Single cards: Success=0.127, Fail=0.315 â†’ Singles lead to more failures
- Pairs: Success=0.174, Fail=0.288 â†’ Pairs also tend to fail
- Straights: Success=0.239, Fail=0.096 â†’ Straights increase success
- Quads: Success=0.273, Fail=0.078 â†’ Quads have highest success rate
```

## âš–ï¸ Conservative Decision Making

### Risk Management Strategy
1. **Double threshold check**: Both prediction==1 AND confidence>=threshold
2. **Dynamic thresholds**: Lower for super strong hands (0.7 vs 0.8)
3. **Rulebase safety net**: Block obviously weak hands first
4. **Balanced class weights**: Reduced from 1:10 to 1:2 for better balance

## ğŸ”§ Model Performance Metrics

### Training Results
- **Training accuracy**: ~0.85-0.90
- **Cross-validation**: 10-fold CV for robustness
- **Precision focus**: Minimize false positives (bad declarations)
- **Recall balance**: Still catch good opportunities

### Hybrid Benefits
1. **Rulebase blocks**: Prevents ML from learning bad synthetic patterns
2. **Conservative thresholds**: Reduces risky declarations
3. **Super strong exceptions**: Allows aggressive play with premium hands
4. **Optimal sequencing**: Maximizes win probability when declaring

## ğŸš¨ Current Limitations

### Data Quality Issues
- **Synthetic bias**: Learning from artificial success_probability rules
- **Limited real gameplay**: No actual opponent behavior patterns
- **Feature redundancy**: 44% dimensions for combo type encoding

### Model Architecture Issues
- **Single tree**: No ensemble diversity
- **Static features**: No game state context
- **Binary classification**: Oversimplified decision space

## ğŸ¯ Solution Strengths

### Architectural Advantages
1. **Hybrid approach**: Combines rule-based safety with ML learning
2. **Conservative design**: Prioritizes precision over recall
3. **Sam-specific**: Tailored strength calculation for Vietnamese Sam
4. **Interpretable**: Decision tree provides explainable predictions

### Practical Benefits
1. **Risk mitigation**: Rulebase prevents obviously bad decisions
2. **Adaptive thresholds**: Different standards for different hand strengths
3. **Optimal sequencing**: Strategic combo ordering when declaring
4. **Extensible**: Easy to add new rules or features

## ğŸ“ˆ Future Improvements

### Short-term Enhancements
1. **Feature selection**: Remove redundant dimensions
2. **Real data integration**: Mix synthetic with actual gameplay logs
3. **Ensemble methods**: Random Forest or Gradient Boosting
4. **Hyperparameter tuning**: Grid search for optimal parameters

### Long-term Roadmap
1. **Contextual features**: Game state, opponent modeling
2. **Sequential modeling**: LSTM/RNN for sequence patterns
3. **Reinforcement learning**: Learn from actual game outcomes
4. **Multi-objective optimization**: Balance multiple success metrics

---

## ğŸ“ Implementation Summary

The Hybrid Conservative Model represents a **pragmatic approach** to AI BÃ¡o SÃ¢m decision making:

- **Safety First**: Rulebase prevents catastrophic mistakes
- **Learning Capability**: ML adapts to patterns in training data  
- **Conservative Bias**: Prioritizes avoiding bad declarations
- **Sam-Optimized**: Strength calculations match Vietnamese Sam rules

While limited by synthetic training data, the architecture provides a **solid foundation** for future enhancements with real gameplay data and more sophisticated ML techniques.